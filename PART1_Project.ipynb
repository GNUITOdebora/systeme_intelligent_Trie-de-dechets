{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnRJnZefNRpu"
   },
   "source": [
    "**Importation des bibliothèques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8hUk3_aCK-1_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_classif\n",
    "import time\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.feature import local_binary_pattern, hog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Evh1DtwOKJlO"
   },
   "source": [
    "**Chargement du Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ib8jxk9Y6j1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des données d'entraînement : (11816, 128, 128, 3)\n",
      "Taille des labels d'entraînement : (11816,)\n",
      "Taille des données de test : (5064, 128, 128, 3)\n",
      "Taille des labels de test : (5064,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Chemins des répertoires\n",
    "train_dir = r\".\\Train\"\n",
    "test_dir = r\".\\Test\"\n",
    "# Fonction pour charger les images et leurs labels\n",
    "def load_data(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label in [\"R\", \"O\"]:\n",
    "        path = os.path.join(directory, label)\n",
    "        for file in os.listdir(path):\n",
    "            img_path = os.path.join(path, file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Redimensionnez l'image pour uniformité\n",
    "                img = cv2.resize(img, (128, 128))  # Taille arbitraire\n",
    "                data.append(img)\n",
    "                labels.append(0 if label == \"R\" else 1)  # 0 pour R, 1 pour O\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Chargement des données\n",
    "X_train, y_train = load_data(train_dir)\n",
    "X_test, y_test = load_data(test_dir)\n",
    "\n",
    "print(f\"Taille des données d'entraînement : {X_train.shape}\")\n",
    "print(f\"Taille des labels d'entraînement : {y_train.shape}\")\n",
    "print(f\"Taille des données de test : {X_test.shape}\")\n",
    "print(f\"Taille des labels de test : {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TotauptKOl8"
   },
   "source": [
    "**Préparation des modéles et leurs evaluations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ijm0sB5QKcwL"
   },
   "outputs": [],
   "source": [
    "# Fonction pour entraîner et évaluer les modèles\n",
    "def train_and_evaluate_models(X_train, X_test, y_train):\n",
    "    # Dictionnaire pour stocker les modèles, les prédictions et les temps\n",
    "    model_results = {}\n",
    "\n",
    "    # 1. Modèle SVM (Support Vector Machine)\n",
    "    svm1 = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "    svm_time_start = time.time()\n",
    "    svm1.fit(X_train, y_train)\n",
    "    svm_time_end = time.time()\n",
    "    pred_svm = svm1.predict(X_test)\n",
    "    model_results['SVM'] = {\n",
    "        'model': svm1,\n",
    "        'predictions': pred_svm,\n",
    "        'time': svm_time_end - svm_time_start\n",
    "    }\n",
    "\n",
    "    # 2. Modèle Random Forest\n",
    "    random_forest1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rd_time_start = time.time()\n",
    "    random_forest1.fit(X_train, y_train)\n",
    "    rd_time_end = time.time()\n",
    "    pred_rf = random_forest1.predict(X_test)\n",
    "    model_results['Random Forest'] = {\n",
    "        'model': random_forest1,\n",
    "        'predictions': pred_rf,\n",
    "        'time': rd_time_end - rd_time_start\n",
    "    }\n",
    "\n",
    "    # 3. Modèle XGBoost\n",
    "    xgb_model1 = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "    xgb_time_start = time.time()\n",
    "    xgb_model1.fit(X_train, y_train)\n",
    "    xgb_time_end = time.time()\n",
    "    pred_xgb = xgb_model1.predict(X_test)\n",
    "    model_results['XGBoost'] = {\n",
    "        'model': xgb_model1,\n",
    "        'predictions': pred_xgb,\n",
    "        'time': xgb_time_end - xgb_time_start\n",
    "    }\n",
    "\n",
    "    # 4. Modèle KNN (K-Nearest Neighbors)\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_time_start = time.time()\n",
    "    knn1.fit(X_train, y_train)\n",
    "    knn_time_end = time.time()\n",
    "    pred_knn = knn1.predict(X_test)\n",
    "    model_results['KNN'] = {\n",
    "        'model': knn1,\n",
    "        'predictions': pred_knn,\n",
    "        'time': knn_time_end - knn_time_start\n",
    "    }\n",
    "\n",
    "    # Retourner les résultats sous forme de dictionnaire\n",
    "    return model_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CT9ZwmhuQSt9"
   },
   "source": [
    "# 1ére Méthode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUtRLgLRJp8A"
   },
   "source": [
    "### **1- Extraction des descripteurs :** \n",
    "* **Méthode Analyse de Texture: \"LBP\"**  \n",
    "\n",
    "* **Techniques de Détection de Contours \"Sobel Operator\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "qUzSZLtm6mQB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques d'entraînement : (11816, 67)\n",
      "Taille des caractéristiques de test : (5064, 67)\n"
     ]
    }
   ],
   "source": [
    "def extract_features(image):\n",
    "    # Conversion en niveaux de gris\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 1. Sobel Operator pour les gradients (détection des contours)\n",
    "    grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)  # Gradient horizontal\n",
    "    grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)  # Gradient vertical\n",
    "    grad_magnitude = cv2.magnitude(grad_x, grad_y)  # Magnitude du gradient\n",
    "    grad_hist = np.histogram(grad_magnitude.ravel(), bins=8, range=(0, 256))[0]\n",
    "    grad_hist = grad_hist.astype(\"float\")\n",
    "    grad_hist /= (grad_hist.sum() + 1e-6)  # Normalisation\n",
    "\n",
    "    # 2. Descripteur LBP (Local Binary Patterns)\n",
    "    radius = 1  # Rayon autour du pixel central\n",
    "    n_points = 8 * radius  # Nombre de voisins autour du pixel central\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')  # Calcul du LBP\n",
    "    lbp_hist = np.histogram(lbp.ravel(), bins=59, range=(0, 59))[0]  # Histogramme des patterns LBP\n",
    "    lbp_hist = lbp_hist.astype(\"float\")\n",
    "    lbp_hist /= (lbp_hist.sum() + 1e-6)  # Normalisation\n",
    "\n",
    "    # Concaténer tous les descripteurs\n",
    "    features = np.hstack([grad_hist, lbp_hist])\n",
    "    return features\n",
    "# Aplatir les images en un seul vecteur pour chaque image\n",
    "def flatten_images(images):\n",
    "    return images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Extraire les caractéristiques pour tout l'ensemble de données\n",
    "def process_dataset(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        # Extraction des caractéristiques pour chaque image\n",
    "        feature = extract_features(img)\n",
    "        features.append(feature)\n",
    "\n",
    "    # Convertir la liste des caractéristiques en un tableau 2D\n",
    "    return np.array(features)\n",
    "# Vérification des formes des données\n",
    "def check_data_shape(X_train, X_test):\n",
    "    print(f\"Taille des caractéristiques d'entraînement : {X_train.shape}\")\n",
    "    print(f\"Taille des caractéristiques de test : {X_test.shape}\")\n",
    "# Extraction des caractéristiques pour l'entraînement et les tests\n",
    "X_train_features = process_dataset(X_train)\n",
    "X_test_features = process_dataset(X_test)\n",
    "\n",
    "# Vérification de la forme des données extraites\n",
    "check_data_shape(X_train_features, X_test_features)\n",
    "# Aplatir les caractéristiques\n",
    "X_train_features_flat = flatten_images(X_train_features)\n",
    "X_test_features_flat = flatten_images(X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyNdkcj3NgP9"
   },
   "source": [
    "### **2-Entrainements  et évaluations des modéles sans Sélection des caracterstiques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HTRKX7WbMjKP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of SVM ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.70      0.73      0.71      2133\n",
      "   Organique       0.80      0.78      0.79      2931\n",
      "\n",
      "    accuracy                           0.76      5064\n",
      "   macro avg       0.75      0.75      0.75      5064\n",
      "weighted avg       0.76      0.76      0.76      5064\n",
      "\n",
      "Temps d'exécution SVM : 31.23304581642151 seconds\n",
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.78      0.75      0.76      2133\n",
      "   Organique       0.82      0.85      0.83      2931\n",
      "\n",
      "    accuracy                           0.81      5064\n",
      "   macro avg       0.80      0.80      0.80      5064\n",
      "weighted avg       0.80      0.81      0.80      5064\n",
      "\n",
      "Temps d'exécution Random Forest : 4.63299560546875 seconds\n",
      "****************************** Performance Evaluation of XGBoost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.78      0.75      0.76      2133\n",
      "   Organique       0.82      0.84      0.83      2931\n",
      "\n",
      "    accuracy                           0.80      5064\n",
      "   macro avg       0.80      0.80      0.80      5064\n",
      "weighted avg       0.80      0.80      0.80      5064\n",
      "\n",
      "Temps d'exécution XGBoost : 0.7039341926574707 seconds\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.72      0.66      0.69      2133\n",
      "   Organique       0.77      0.81      0.79      2931\n",
      "\n",
      "    accuracy                           0.75      5064\n",
      "   macro avg       0.74      0.74      0.74      5064\n",
      "weighted avg       0.75      0.75      0.75      5064\n",
      "\n",
      "Temps d'exécution KNN : 0.003998279571533203 seconds\n"
     ]
    }
   ],
   "source": [
    "model_results_sans_selection = train_and_evaluate_models(X_train_features_flat, X_test_features_flat, y_train)\n",
    "# Afficher les résultats pour chaque modèle\n",
    "for model_name, result in model_results_sans_selection.items():\n",
    "    print(f\"****************************** Performance Evaluation of {model_name} ****************************************\")\n",
    "    print(classification_report(y_test, result['predictions'], target_names=[\"Recyclable\", \"Organique\"]))\n",
    "    print(f\"Temps d'exécution {model_name} : {result['time']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NK11gf6WN4yD"
   },
   "source": [
    "### **3- Sélection de caractéristiques (feature selection)**\n",
    "\n",
    "C'est une étape cruciale dans le processus de prétraitement des données, en particulier pour les modèles d'apprentissage automatique. Elle permet d'améliorer les performances des modèles en réduisant la dimensionnalité, en supprimant les informations redondantes ou inutiles et en réduisant le risque de surajustement (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYKLeG2LOI1y"
   },
   "source": [
    "#### **3.1. Sélection par Modèle (Embedded Method)**\n",
    "\n",
    "**Objectif** : Évaluer l'importance des caractéristiques à partir du modèle d'apprentissage.\n",
    "\n",
    "**Méthode** :\n",
    "* Utiliser un modèle comme Random Forest pour obtenir l'importance de chaque caractéristique.\n",
    "* Sélectionner les caractéristiques ayant la plus grande importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxK_thk0OlnT"
   },
   "source": [
    "**1- Sélection par Modèle (Embedded Method) avec RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oJ0AJGlULjnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques après sélection par modèle : (11816, 18)\n"
     ]
    }
   ],
   "source": [
    "# Sélection par Modèle (Embedded Method) avec RandomForest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_features_flat, y_train)\n",
    "# Sélectionner les caractéristiques en fonction de l'importance calculée par RandomForest\n",
    "select_from_model = SelectFromModel(rf_model, threshold=\"mean\", max_features=20)  # Sélectionner les 20 meilleures caractéristiques\n",
    "X_train_selected = select_from_model.transform(X_train_features_flat)\n",
    "X_test_selected = select_from_model.transform(X_test_features_flat)\n",
    "print(f\"Taille des caractéristiques après sélection par modèle : {X_train_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vScBMR_uMxfX"
   },
   "source": [
    "**2- Entrainement  et evaluation des modéles avec la Séléction des caractérstiques avec la méthode embedded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Whzh3ro1LzQo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of SVM ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.70      0.73      0.71      2133\n",
      "   Organique       0.80      0.78      0.79      2931\n",
      "\n",
      "    accuracy                           0.76      5064\n",
      "   macro avg       0.75      0.75      0.75      5064\n",
      "weighted avg       0.76      0.76      0.76      5064\n",
      "\n",
      "Temps d'exécution SVM : 26.558998823165894 seconds\n",
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.78      0.76      0.77      2133\n",
      "   Organique       0.83      0.85      0.84      2931\n",
      "\n",
      "    accuracy                           0.81      5064\n",
      "   macro avg       0.81      0.80      0.80      5064\n",
      "weighted avg       0.81      0.81      0.81      5064\n",
      "\n",
      "Temps d'exécution Random Forest : 7.016042232513428 seconds\n",
      "****************************** Performance Evaluation of XGBoost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.78      0.75      0.76      2133\n",
      "   Organique       0.82      0.84      0.83      2931\n",
      "\n",
      "    accuracy                           0.80      5064\n",
      "   macro avg       0.80      0.80      0.80      5064\n",
      "weighted avg       0.80      0.80      0.80      5064\n",
      "\n",
      "Temps d'exécution XGBoost : 0.5419974327087402 seconds\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.72      0.66      0.69      2133\n",
      "   Organique       0.77      0.81      0.79      2931\n",
      "\n",
      "    accuracy                           0.75      5064\n",
      "   macro avg       0.74      0.74      0.74      5064\n",
      "weighted avg       0.75      0.75      0.75      5064\n",
      "\n",
      "Temps d'exécution KNN : 0.004997730255126953 seconds\n"
     ]
    }
   ],
   "source": [
    "model_results_avec_selection = train_and_evaluate_models(X_train_selected, X_test_selected, y_train)\n",
    "# Afficher les résultats pour chaque modèle\n",
    "for model_name, result in model_results_avec_selection.items():\n",
    "    print(f\"****************************** Performance Evaluation of {model_name} ****************************************\")\n",
    "    print(classification_report(y_test, result['predictions'], target_names=[\"Recyclable\", \"Organique\"]))\n",
    "    print(f\"Temps d'exécution {model_name} : {result['time']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIcdSfELOW_J"
   },
   "source": [
    "#### **3.2  Sélection par Test Statistique (Filter Method)**\n",
    "\n",
    "Les tests statistiques, comme le test du chi carré (chi-squared test) ou **ANOVA**, sont utilisés pour évaluer la relation entre chaque caractéristique et la cible.\n",
    "\n",
    "**Objectif** : Sélectionner les caractéristiques ayant une forte relation statistique avec la cible.\n",
    "\n",
    "**Méthode** :\n",
    "\n",
    "* Appliquer un test statistique pour mesurer la dépendance entre chaque caractéristique et la variable cible.\n",
    "* Sélectionner les caractéristiques avec les valeurs de test les plus significatives.[texte du lien](https://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyEueUGIOqrS"
   },
   "source": [
    "**1- Sélection par Test Statistique (Filter Method) avec SelectKBest et ANOVA F-test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mTIjZz1aOcNH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques après sélection par test statistique : (11816, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\debgn\\anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41\n",
      " 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65\n",
      " 66] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\debgn\\anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# Sélection par Test Statistique (Filter Method) avec SelectKBest et ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=20)  # Sélectionner les 20 meilleures caractéristiques\n",
    "X_train_selected_filter = selector.fit_transform(X_train_features_flat, y_train)\n",
    "X_test_selected_filter = selector.transform(X_test_features_flat)\n",
    "print(f\"Taille des caractéristiques après sélection par test statistique : {X_train_selected_filter.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJtH81syO3KU"
   },
   "source": [
    "**2- Entrainement  et evaluation des modéles avec la Séléction des caractérstiques avec la méthode Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "cWbpGVI6O7vB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of SVM ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.70      0.73      0.71      2133\n",
      "   Organique       0.80      0.78      0.79      2931\n",
      "\n",
      "    accuracy                           0.76      5064\n",
      "   macro avg       0.75      0.75      0.75      5064\n",
      "weighted avg       0.76      0.76      0.76      5064\n",
      "\n",
      "Temps d'exécution SVM : 26.832210779190063 seconds\n",
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.78      0.75      0.77      2133\n",
      "   Organique       0.82      0.84      0.83      2931\n",
      "\n",
      "    accuracy                           0.81      5064\n",
      "   macro avg       0.80      0.80      0.80      5064\n",
      "weighted avg       0.81      0.81      0.81      5064\n",
      "\n",
      "Temps d'exécution Random Forest : 6.718001842498779 seconds\n",
      "****************************** Performance Evaluation of XGBoost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.78      0.75      0.76      2133\n",
      "   Organique       0.82      0.84      0.83      2931\n",
      "\n",
      "    accuracy                           0.80      5064\n",
      "   macro avg       0.80      0.80      0.80      5064\n",
      "weighted avg       0.80      0.80      0.80      5064\n",
      "\n",
      "Temps d'exécution XGBoost : 0.5679988861083984 seconds\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.72      0.66      0.69      2133\n",
      "   Organique       0.77      0.81      0.79      2931\n",
      "\n",
      "    accuracy                           0.75      5064\n",
      "   macro avg       0.74      0.74      0.74      5064\n",
      "weighted avg       0.75      0.75      0.75      5064\n",
      "\n",
      "Temps d'exécution KNN : 0.0060002803802490234 seconds\n"
     ]
    }
   ],
   "source": [
    "model_results_avec_selection_filter = train_and_evaluate_models(X_train_selected_filter, X_test_selected_filter, y_train)\n",
    "# Afficher les résultats pour chaque modèle\n",
    "for model_name, result in model_results_avec_selection_filter.items():\n",
    "    print(f\"****************************** Performance Evaluation of {model_name} ****************************************\")\n",
    "    print(classification_report(y_test, result['predictions'], target_names=[\"Recyclable\", \"Organique\"]))\n",
    "    print(f\"Temps d'exécution {model_name} : {result['time']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2éme Méthode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1- Extraction des descripteurs :**\n",
    "* **Méthode Analyse de Texture: \"Gabor\"** \n",
    "*  **techniques de Détection de Contours \"Scharr\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques d'entraînement : (11816, 48)\n",
      "Taille des caractéristiques de test : (5064, 48)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.filters import gabor\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fonction pour extraire les descripteurs Scharr et Gabor\n",
    "def extract_features(image):\n",
    "    # Conversion en niveaux de gris\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 1. Détection de contours avec les filtres de Scharr\n",
    "    scharr_x = cv2.Scharr(gray, cv2.CV_64F, 1, 0)  # Dérivée horizontale\n",
    "    scharr_y = cv2.Scharr(gray, cv2.CV_64F, 0, 1)  # Dérivée verticale\n",
    "    scharr_magnitude = cv2.magnitude(scharr_x, scharr_y)  # Magnitude des gradients de Scharr\n",
    "    scharr_hist = np.histogram(scharr_magnitude.ravel(), bins=8, range=(0, 256))[0]  # Histogramme\n",
    "    scharr_hist = scharr_hist.astype(\"float\")\n",
    "    scharr_hist /= (scharr_hist.sum() + 1e-6)  # Normalisation\n",
    "    \n",
    "    # 2. Analyse de texture avec les filtres de Gabor\n",
    "    # Application de plusieurs filtres de Gabor à différentes fréquences et orientations\n",
    "    num_filters = 5  # Nombre de filtres de Gabor à différentes orientations et échelles\n",
    "    gabor_features = []\n",
    "    for theta in np.arange(0, np.pi, np.pi / num_filters):  # Différentes orientations\n",
    "        real, imag = gabor(gray, frequency=0.6, theta=theta)\n",
    "        gabor_mag = np.sqrt(real**2 + imag**2)\n",
    "        gabor_hist = np.histogram(gabor_mag.ravel(), bins=8, range=(0, 256))[0]  # Histogramme des magnitudes\n",
    "        gabor_hist = gabor_hist.astype(\"float\")\n",
    "        gabor_hist /= (gabor_hist.sum() + 1e-6)  # Normalisation\n",
    "        gabor_features.append(gabor_hist)\n",
    "    \n",
    "    # Concaténer tous les descripteurs (Scharr + Gabor)\n",
    "    features = np.hstack([scharr_hist] + gabor_features)\n",
    "    return features\n",
    "\n",
    "# Aplatir les images en un seul vecteur pour chaque image\n",
    "def flatten_images(images):\n",
    "    return images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Extraire les caractéristiques pour tout l'ensemble de données\n",
    "def process_dataset(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        # Extraction des caractéristiques pour chaque image\n",
    "        feature = extract_features(img)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # Convertir la liste des caractéristiques en un tableau 2D\n",
    "    return np.array(features)\n",
    "\n",
    "# Vérification des formes des données\n",
    "def check_data_shape(X_train, X_test):\n",
    "    print(f\"Taille des caractéristiques d'entraînement : {X_train.shape}\")\n",
    "    print(f\"Taille des caractéristiques de test : {X_test.shape}\")\n",
    "\n",
    "\n",
    "# Extraction des caractéristiques pour l'entraînement et les tests\n",
    "X_train_features = process_dataset(X_train)\n",
    "X_test_features = process_dataset(X_test)\n",
    "\n",
    "# Vérification de la forme des données extraites\n",
    "check_data_shape(X_train_features, X_test_features)\n",
    "\n",
    "# Aplatir les caractéristiques\n",
    "X_train_features_flat_methode3 = flatten_images(X_train_features)\n",
    "X_test_features_flat_methode3= flatten_images(X_test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2- Entrainements  et évaluations des modéles sans Sélection des caracterstiques**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of SVM ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.64      0.70      0.67      2133\n",
      "   Organique       0.77      0.71      0.74      2931\n",
      "\n",
      "    accuracy                           0.71      5064\n",
      "   macro avg       0.70      0.71      0.70      5064\n",
      "weighted avg       0.71      0.71      0.71      5064\n",
      "\n",
      "Temps d'exécution SVM : 34.83799719810486 seconds\n",
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.71      0.63      0.67      2133\n",
      "   Organique       0.75      0.81      0.78      2931\n",
      "\n",
      "    accuracy                           0.73      5064\n",
      "   macro avg       0.73      0.72      0.72      5064\n",
      "weighted avg       0.73      0.73      0.73      5064\n",
      "\n",
      "Temps d'exécution Random Forest : 4.4590065479278564 seconds\n",
      "****************************** Performance Evaluation of XGBoost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.70      0.62      0.66      2133\n",
      "   Organique       0.74      0.80      0.77      2931\n",
      "\n",
      "    accuracy                           0.73      5064\n",
      "   macro avg       0.72      0.71      0.71      5064\n",
      "weighted avg       0.72      0.73      0.72      5064\n",
      "\n",
      "Temps d'exécution XGBoost : 0.5620007514953613 seconds\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.68      0.62      0.65      2133\n",
      "   Organique       0.74      0.78      0.76      2931\n",
      "\n",
      "    accuracy                           0.72      5064\n",
      "   macro avg       0.71      0.70      0.71      5064\n",
      "weighted avg       0.71      0.72      0.71      5064\n",
      "\n",
      "Temps d'exécution KNN : 0.0029993057250976562 seconds\n"
     ]
    }
   ],
   "source": [
    "model_results_methode3 = train_and_evaluate_models(X_train_features_flat_methode3, X_test_features_flat_methode3, y_train)\n",
    "# Afficher les résultats pour chaque modèle\n",
    "for model_name, result in model_results_methode3.items():\n",
    "    print(f\"****************************** Performance Evaluation of {model_name} ****************************************\")\n",
    "    print(classification_report(y_test, result['predictions'], target_names=[\"Recyclable\", \"Organique\"]))\n",
    "    print(f\"Temps d'exécution {model_name} : {result['time']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3- Sélection de caractéristiques (feature selection)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1- Sélection par Modèle (Embedded Method) avec RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques après sélection par modèle : (11816, 8)\n"
     ]
    }
   ],
   "source": [
    "# Sélection par Modèle (Embedded Method) avec RandomForest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_features_flat_methode3, y_train)\n",
    "# Sélectionner les caractéristiques en fonction de l'importance calculée par RandomForest\n",
    "select_from_model = SelectFromModel(rf_model, threshold=\"mean\", max_features=20)  # Sélectionner les 20 meilleures caractéristiques\n",
    "X_train_selected_methode3 = select_from_model.transform(X_train_features_flat_methode3)\n",
    "X_test_selected_methode3 = select_from_model.transform(X_test_features_flat_methode3)\n",
    "print(f\"Taille des caractéristiques après sélection par modèle : {X_train_selected_methode3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrainement  et evaluation des modéles avec la Séléction des caractérstiques avec la méthode embedded**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of SVM ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.64      0.70      0.67      2133\n",
      "   Organique       0.77      0.71      0.74      2931\n",
      "\n",
      "    accuracy                           0.71      5064\n",
      "   macro avg       0.70      0.71      0.70      5064\n",
      "weighted avg       0.71      0.71      0.71      5064\n",
      "\n",
      "Temps d'exécution SVM : 30.72598171234131 seconds\n",
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.71      0.63      0.67      2133\n",
      "   Organique       0.75      0.81      0.78      2931\n",
      "\n",
      "    accuracy                           0.73      5064\n",
      "   macro avg       0.73      0.72      0.72      5064\n",
      "weighted avg       0.73      0.73      0.73      5064\n",
      "\n",
      "Temps d'exécution Random Forest : 4.8829731941223145 seconds\n",
      "****************************** Performance Evaluation of XGBoost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.70      0.62      0.66      2133\n",
      "   Organique       0.74      0.80      0.77      2931\n",
      "\n",
      "    accuracy                           0.73      5064\n",
      "   macro avg       0.72      0.71      0.71      5064\n",
      "weighted avg       0.72      0.73      0.72      5064\n",
      "\n",
      "Temps d'exécution XGBoost : 0.3359708786010742 seconds\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.68      0.62      0.65      2133\n",
      "   Organique       0.74      0.78      0.76      2931\n",
      "\n",
      "    accuracy                           0.72      5064\n",
      "   macro avg       0.71      0.70      0.71      5064\n",
      "weighted avg       0.71      0.72      0.71      5064\n",
      "\n",
      "Temps d'exécution KNN : 0.05700087547302246 seconds\n"
     ]
    }
   ],
   "source": [
    "model_results_avec_selection = train_and_evaluate_models(X_train_selected_methode3, X_test_selected_methode3, y_train)\n",
    "# Afficher les résultats pour chaque modèle\n",
    "for model_name, result in model_results_avec_selection.items():\n",
    "    print(f\"****************************** Performance Evaluation of {model_name} ****************************************\")\n",
    "    print(classification_report(y_test, result['predictions'], target_names=[\"Recyclable\", \"Organique\"]))\n",
    "    print(f\"Temps d'exécution {model_name} : {result['time']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 - Sélection par Test Statistique (Filter Method) avec SelectKBest et ANOVA F-test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques après sélection par test statistique : (11816, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\debgn\\anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [ 9 10 11 12 13 14 15 17 18 19 20 21 22 23 25 26 27 28 29 30 31 33 34 35\n",
      " 36 37 38 39 41 42 43 44 45 46 47] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\debgn\\anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# Sélection par Test Statistique (Filter Method) avec SelectKBest et ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=20)  # Sélectionner les 20 meilleures caractéristiques\n",
    "X_train_selected_filter = selector.fit_transform(X_train_features_flat_methode3, y_train)\n",
    "X_test_selected_filter = selector.transform(X_test_features_flat_methode3)\n",
    "print(f\"Taille des caractéristiques après sélection par test statistique : {X_train_selected_filter.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrainement  et evaluation des modéles avec la Séléction des caractérstiques avec la méthode Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of SVM ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.64      0.70      0.67      2133\n",
      "   Organique       0.77      0.71      0.74      2931\n",
      "\n",
      "    accuracy                           0.71      5064\n",
      "   macro avg       0.70      0.71      0.70      5064\n",
      "weighted avg       0.71      0.71      0.71      5064\n",
      "\n",
      "Temps d'exécution SVM : 31.833030939102173 seconds\n",
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.70      0.63      0.66      2133\n",
      "   Organique       0.75      0.81      0.78      2931\n",
      "\n",
      "    accuracy                           0.73      5064\n",
      "   macro avg       0.72      0.72      0.72      5064\n",
      "weighted avg       0.73      0.73      0.73      5064\n",
      "\n",
      "Temps d'exécution Random Forest : 4.54604434967041 seconds\n",
      "****************************** Performance Evaluation of XGBoost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.70      0.62      0.66      2133\n",
      "   Organique       0.74      0.80      0.77      2931\n",
      "\n",
      "    accuracy                           0.73      5064\n",
      "   macro avg       0.72      0.71      0.71      5064\n",
      "weighted avg       0.72      0.73      0.72      5064\n",
      "\n",
      "Temps d'exécution XGBoost : 0.34999704360961914 seconds\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.68      0.62      0.65      2133\n",
      "   Organique       0.74      0.78      0.76      2931\n",
      "\n",
      "    accuracy                           0.72      5064\n",
      "   macro avg       0.71      0.70      0.71      5064\n",
      "weighted avg       0.71      0.72      0.71      5064\n",
      "\n",
      "Temps d'exécution KNN : 0.005002260208129883 seconds\n"
     ]
    }
   ],
   "source": [
    "model_results_avec_selection_filter = train_and_evaluate_models(X_train_selected_filter, X_test_selected_filter, y_train)\n",
    "# Afficher les résultats pour chaque modèle\n",
    "for model_name, result in model_results_avec_selection_filter.items():\n",
    "    print(f\"****************************** Performance Evaluation of {model_name} ****************************************\")\n",
    "    print(classification_report(y_test, result['predictions'], target_names=[\"Recyclable\", \"Organique\"]))\n",
    "    print(f\"Temps d'exécution {model_name} : {result['time']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3éme Méthode (ajout d'un descripteur de couleur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1- Extraction des descripteurs :**\n",
    "* **Méthode Analyse de Texture: \"GLCM\"** \n",
    "*  **techniques de Détection de Contours \"Canny\"**\n",
    "* **Histogramme de couleurs (HSV) pour extraire les couleur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import  graycomatrix, graycoprops\n",
    "\n",
    "# Fonction pour extraire les descripteurs (GLCM, HSV, et Canny)\n",
    "def extract_features(image):\n",
    "    # Conversion en niveaux de gris\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 1. Canny (Contours)\n",
    "    edges = cv2.Canny(gray, threshold1=100, threshold2=200)\n",
    "    edges_hist = np.histogram(edges.ravel(), bins=2, range=(0, 2))[0]  # Histogramme des bords\n",
    "    edges_hist = edges_hist.astype(\"float\")\n",
    "    edges_hist /= (edges_hist.sum() + 1e-6)  # Normalisation\n",
    "    \n",
    "    # 2. Histogramme de couleurs (HSV)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten()\n",
    "    hist /= (hist.sum() + 1e-6)  # Normalisation\n",
    "    \n",
    "    # 3. GLCM (Gray-Level Co-occurrence Matrix) pour la texture\n",
    "    glcm = graycomatrix(gray, distances=[5], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "    \n",
    "    # Concaténer les descripteurs\n",
    "    features = np.hstack([hist, edges_hist, contrast, homogeneity])\n",
    "    return features\n",
    "\n",
    "# Fonction pour aplatir les données\n",
    "def flatten_images(images):\n",
    "    return images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Extraire les caractéristiques pour tout l'ensemble de données\n",
    "def process_dataset(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        # Extraction des caractéristiques pour chaque image\n",
    "        feature = extract_features(img)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # Convertir la liste des caractéristiques en un tableau 2D\n",
    "    return np.array(features)\n",
    "\n",
    "# Vérification des formes des données\n",
    "def check_data_shape(X_train, X_test):\n",
    "    print(f\"Taille des caractéristiques d'entraînement : {X_train.shape}\")\n",
    "    print(f\"Taille des caractéristiques de test : {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques d'entraînement : (11816, 516)\n",
      "Taille des caractéristiques de test : (5064, 516)\n",
      "********************* apres flatten ***********************\n",
      "Taille des caractéristiques d'entraînement : (11816, 516)\n",
      "Taille des caractéristiques de test : (5064, 516)\n"
     ]
    }
   ],
   "source": [
    "# Extraction des caractéristiques pour l'entraînement et les tests\n",
    "X_train_features = process_dataset(X_train)\n",
    "X_test_features = process_dataset(X_test)\n",
    "\n",
    "# Vérification de la forme des données extraites\n",
    "check_data_shape(X_train_features, X_test_features)\n",
    "\n",
    "# Aplatir les caractéristiques (si nécessaire)\n",
    "X_train_features = flatten_images(X_train_features)\n",
    "X_test_features = flatten_images(X_test_features)\n",
    "\n",
    "# Vérification des formes après aplatissement\n",
    "print(\"********************* apres flatten ***********************\")\n",
    "check_data_shape(X_train_features, X_test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2-Entrainements  et évaluations des modéles sans Sélection des caracterstiques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "# 1. Modèle KNN (K-Nearest Neighbors)\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5)\n",
    "# Entraînement des modèles\n",
    "knn_time_start= time.time()\n",
    "knn1.fit(X_train_features, y_train)\n",
    "knn_time_end= time.time()\n",
    "# Faire des prédictions\n",
    "pred_knn = knn1.predict(X_test_features)\n",
    "\n",
    "\n",
    "# 2. Modèle RandomForest\n",
    "random_forest1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Entraînement des modèles\n",
    "rd_time_start= time.time()\n",
    "random_forest1.fit(X_train_features, y_train)\n",
    "rd_time_end= time.time()\n",
    "# Faire des prédictions\n",
    "pred_rf = random_forest1.predict(X_test_features)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Modèle XGBoost\n",
    "xgb_model1 = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "# Entraînement des modèles\n",
    "xgb_time_start= time.time()\n",
    "xgb_model1.fit(X_train_features, y_train)\n",
    "xgb_time_end= time.time()\n",
    "# Faire des prédictions\n",
    "pred_xgb = xgb_model1.predict(X_test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.88      0.80      0.84      2133\n",
      "   Organique       0.86      0.92      0.89      2931\n",
      "\n",
      "    accuracy                           0.87      5064\n",
      "   macro avg       0.87      0.86      0.86      5064\n",
      "weighted avg       0.87      0.87      0.87      5064\n",
      "\n",
      "le temp d'execution : 10.53600001335144\n",
      "****************************** Performance Evaluation of  XGboost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.87      0.82      0.85      2133\n",
      "   Organique       0.88      0.91      0.89      2931\n",
      "\n",
      "    accuracy                           0.87      5064\n",
      "   macro avg       0.87      0.87      0.87      5064\n",
      "weighted avg       0.87      0.87      0.87      5064\n",
      "\n",
      "le temp d'execution : 5.494972467422485\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.55      0.43      0.48      2133\n",
      "   Organique       0.64      0.75      0.69      2931\n",
      "\n",
      "    accuracy                           0.61      5064\n",
      "   macro avg       0.60      0.59      0.59      5064\n",
      "weighted avg       0.61      0.61      0.60      5064\n",
      "\n",
      "le temp d'execution des: 0.02099299430847168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"****************************** Performance Evaluation of Random Forest ****************************************\")\n",
    "print(classification_report(y_test, pred_rf, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution :\",rd_time_end-rd_time_start)\n",
    "\n",
    "\n",
    "print(\"****************************** Performance Evaluation of  XGboost ****************************************\")\n",
    "print(classification_report(y_test, pred_xgb, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution :\",xgb_time_end-xgb_time_start)\n",
    "\n",
    "\n",
    "print(\"****************************** Performance Evaluation of KNN ****************************************\")\n",
    "print(classification_report(y_test, pred_knn, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution des:\",knn_time_end-knn_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3- Sélection de caractéristiques (feature selection)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1- Sélection par Modèle (Embedded Method) avec RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques de train apres selection des caractéeistique : (11816, 50)\n",
      "Taille des caractéristiques de test apres selection des caractéeistique : (5064, 50)\n"
     ]
    }
   ],
   "source": [
    "# Récupérer l'importance des caractéristiques du model random forest\n",
    "importances = random_forest1.feature_importances_\n",
    "\n",
    "# Trier les caractéristiques selon leur importance\n",
    "indices = np.argsort(importances)[::-1]  # Trier par ordre décroissant\n",
    "X_train_selected1 = X_train_features[:, indices[:50]]  # Garder les 10 meilleures caractéristiques\n",
    "X_test_selected1= X_test_features[:, indices[:50]]  # Garder les 10 meilleures caractéristiques\n",
    "\n",
    "print(f\"Taille des caractéristiques de train apres selection des caractéeistique : {X_train_selected1.shape}\")\n",
    "print(f\"Taille des caractéristiques de test apres selection des caractéeistique : {X_test_selected1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apprentissage apres selection des caractéristiques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modèle RandomForest\n",
    "random_forest2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Modèle SVC (Support Vector Machine)\n",
    "\n",
    "# 3. Modèle XGBoost\n",
    "\n",
    "xgb_model2 = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 4. Modèle KNN (K-Nearest Neighbors)\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Entraînement des modèles\n",
    "rd_time_start= time.time()\n",
    "random_forest2.fit(X_train_selected1, y_train)\n",
    "rd_time_end= time.time()\n",
    "\n",
    "\n",
    "xgb_time_start= time.time()\n",
    "xgb_model2.fit(X_train_selected1, y_train)\n",
    "xgb_time_end= time.time()\n",
    "\n",
    "knn_time_start= time.time()\n",
    "knn2.fit(X_train_selected1, y_train)\n",
    "knn_time_end= time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Faire des prédictions\n",
    "\n",
    "pred_rf = random_forest2.predict(X_test_selected1)\n",
    "pred_xgb = xgb_model2.predict(X_test_selected1)\n",
    "pred_knn = knn2.predict(X_test_selected1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.85      0.78      0.81      2133\n",
      "   Organique       0.85      0.90      0.87      2931\n",
      "\n",
      "    accuracy                           0.85      5064\n",
      "   macro avg       0.85      0.84      0.84      5064\n",
      "weighted avg       0.85      0.85      0.85      5064\n",
      "\n",
      "le temp d'execution : 6.359045743942261\n",
      "****************************** Performance Evaluation of  XGboost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.84      0.80      0.82      2133\n",
      "   Organique       0.86      0.89      0.87      2931\n",
      "\n",
      "    accuracy                           0.85      5064\n",
      "   macro avg       0.85      0.84      0.85      5064\n",
      "weighted avg       0.85      0.85      0.85      5064\n",
      "\n",
      "le temp d'execution : 0.9649510383605957\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.74      0.77      0.76      2133\n",
      "   Organique       0.83      0.81      0.82      2931\n",
      "\n",
      "    accuracy                           0.79      5064\n",
      "   macro avg       0.79      0.79      0.79      5064\n",
      "weighted avg       0.79      0.79      0.79      5064\n",
      "\n",
      "le temp d'execution des: 0.007004261016845703\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "print(\"****************************** Performance Evaluation of Random Forest ****************************************\")\n",
    "print(classification_report(y_test, pred_rf, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution :\",rd_time_end-rd_time_start)\n",
    "\n",
    "\n",
    "print(\"****************************** Performance Evaluation of  XGboost ****************************************\")\n",
    "print(classification_report(y_test, pred_xgb, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution :\",xgb_time_end-xgb_time_start)\n",
    "\n",
    "\n",
    "print(\"****************************** Performance Evaluation of KNN ****************************************\")\n",
    "print(classification_report(y_test, pred_knn, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution des:\",knn_time_end-knn_time_start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 - Sélection par Test Statistique (Filter Method) avec SelectKBest et ANOVA F-test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille des caractéristiques de train apres selection des caractéeistique : (11816, 50)\n",
      "Taille des caractéristiques de test apres selection des caractéeistique : (5064, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\debgn\\anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437\n",
      " 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n",
      " 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491\n",
      " 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509\n",
      " 510 511 513] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\debgn\\anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Utiliser ANOVA pour sélectionner les meilleures caractéristiques\n",
    "selector = SelectKBest(f_classif, k=50)  # Vous pouvez ajuster k pour sélectionner un nombre spécifique de caractéristiques\n",
    "X_train_selected2 = selector.fit_transform(X_train_features, y_train)\n",
    "X_test_selected2 = selector.transform(X_test_features)\n",
    "\n",
    "\n",
    "\n",
    "anova_scores = selector.scores_\n",
    "print(f\"Taille des caractéristiques de train apres selection des caractéeistique : {X_train_selected2.shape}\")\n",
    "print(f\"Taille des caractéristiques de test apres selection des caractéeistique : {X_test_selected2.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apprentissage apres selection des caractéristiques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modèle RandomForest\n",
    "random_forest3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Modèle SVC (Support Vector Machine)\n",
    "\n",
    "# 3. Modèle XGBoost\n",
    "\n",
    "xgb_model3 = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 4. Modèle KNN (K-Nearest Neighbors)\n",
    "knn3 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Entraînement des modèles\n",
    "rd_time_start= time.time()\n",
    "random_forest3.fit(X_train_selected2, y_train)\n",
    "rd_time_end= time.time()\n",
    "\n",
    "\n",
    "\n",
    "xgb_time_start= time.time()\n",
    "xgb_model3.fit(X_train_selected2, y_train)\n",
    "xgb_time_end= time.time()\n",
    "\n",
    "knn_time_start= time.time()\n",
    "knn3.fit(X_train_selected2, y_train)\n",
    "knn_time_end= time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Faire des prédictions\n",
    "\n",
    "pred_rf = random_forest3.predict(X_test_selected2)\n",
    "pred_xgb = xgb_model3.predict(X_test_selected2)\n",
    "pred_knn = knn3.predict(X_test_selected2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Performance Evaluation of Random Forest ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.85      0.77      0.81      2133\n",
      "   Organique       0.84      0.90      0.87      2931\n",
      "\n",
      "    accuracy                           0.85      5064\n",
      "   macro avg       0.85      0.84      0.84      5064\n",
      "weighted avg       0.85      0.85      0.85      5064\n",
      "\n",
      "le temp d'execution : 6.566993474960327\n",
      "****************************** Performance Evaluation of  XGboost ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.83      0.79      0.81      2133\n",
      "   Organique       0.85      0.89      0.87      2931\n",
      "\n",
      "    accuracy                           0.84      5064\n",
      "   macro avg       0.84      0.84      0.84      5064\n",
      "weighted avg       0.84      0.84      0.84      5064\n",
      "\n",
      "le temp d'execution : 1.1289989948272705\n",
      "****************************** Performance Evaluation of KNN ****************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Recyclable       0.74      0.79      0.76      2133\n",
      "   Organique       0.84      0.80      0.82      2931\n",
      "\n",
      "    accuracy                           0.79      5064\n",
      "   macro avg       0.79      0.79      0.79      5064\n",
      "weighted avg       0.80      0.79      0.80      5064\n",
      "\n",
      "le temp d'execution des: 0.008999109268188477\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "\n",
    "print(\"****************************** Performance Evaluation of Random Forest ****************************************\")\n",
    "print(classification_report(y_test, pred_rf, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution :\",rd_time_end-rd_time_start)\n",
    "\n",
    "\n",
    "print(\"****************************** Performance Evaluation of  XGboost ****************************************\")\n",
    "print(classification_report(y_test, pred_xgb, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution :\",xgb_time_end-xgb_time_start)\n",
    "\n",
    "\n",
    "print(\"****************************** Performance Evaluation of KNN ****************************************\")\n",
    "print(classification_report(y_test, pred_knn, target_names=[\"Recyclable\", \"Organique\"]))\n",
    "print(\"le temp d'execution des:\",knn_time_end-knn_time_start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "Dans le cadre de ce travail, nous avons exploré trois combinaisons distinctes de descripteurs, en utilisant deux approches différentes pour la sélection des caractéristiques : la sélection **basée sur les modèles** et la sélection basée sur des **tests statistiques**.\n",
    "\n",
    "**Points clés :**\n",
    "* **Impact de la sélection des caractéristiques :**\n",
    "La sélection des caractéristiques s'est avérée bénéfique, non seulement pour améliorer **la précision** de la classification, mais également pour réduire considérablement le **temps d'apprentissage** des modèles. Cela a permis de diminuer la complexité des données tout en préservant les informations essentielles.\n",
    "\n",
    "* **Importance des descripteurs de couleur :**\n",
    "Nous avons constaté une amélioration significative des performances lorsque nous avons intégré un descripteur de couleur, notamment l'histogramme en espace de couleurs HSV. Ce descripteur a enrichi la représentation des images en capturant des informations pertinentes sur les nuances et intensités, essentielles pour différencier les classes.\n",
    "\n",
    "\n",
    "L'optimisation des caractéristiques, tant par le choix des descripteurs que par leur sélection stratégique, est une étape cruciale pour obtenir des modèles performants et efficients. Ce travail met en évidence l'importance d'une approche combinée, en tirant parti de la complémentarité entre descripteurs de texture, de contour et de couleur, pour maximiser les résultats."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
